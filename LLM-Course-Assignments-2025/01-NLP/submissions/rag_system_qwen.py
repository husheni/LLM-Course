#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºé˜¿é‡Œäº‘é€šä¹‰åƒé—®çš„RAGç³»ç»Ÿå®ç°
æ”¯æŒå¤šå±‚çº§æ–‡æœ¬åˆ‡ç‰‡ã€è¯­ä¹‰æ£€ç´¢å’Œç›¸ä¼¼åº¦è®¡ç®—
"""
import hashlib
import json
import os
import random

import numpy as np
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass

import requests
from dashscope import Generation
from dashscope.api_entities.dashscope_response import GenerationResponse
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
import logging
from deep_translator import GoogleTranslator
# å°è¯•å¯¼å…¥é˜¿é‡Œäº‘ç›¸å…³ç»„ä»¶
try:
    import dashscope
    # ç›´æ¥ä½¿ç”¨ dashscope è€Œä¸æ˜¯ langchain åŒ…è£…å™¨
    ALIBABA_CLOUD_AVAILABLE = True
except ImportError:
    ALIBABA_CLOUD_AVAILABLE = False

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DashScopeEmbeddingWrapper:
    """DashScope Embedding å°è£…ç±»"""
    
    def __init__(self, model: str = "text-embedding-v1"):
        self.model = model
        self.dashscope = dashscope
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡è·å–æ–‡æ¡£embedding"""
        try:
            response = self.dashscope.TextEmbedding.call(
                model=self.model,
                input=texts,
                text_type="document"
            )
            if response and hasattr(response, 'output') and hasattr(response.output, 'embeddings'):
                embeddings = []
                for item in response.output.embeddings:
                    if hasattr(item, 'embedding'):
                        embeddings.append(item.embedding)
                return embeddings
            else:
                logger.error("è·å–embeddingå¤±è´¥")
                return []
        except Exception as e:
            logger.error(f"DashScope embeddingè°ƒç”¨å¤±è´¥: {e}")
            return []
    
    def embed_query(self, text: str) -> List[float]:
        """è·å–æŸ¥è¯¢embedding"""
        try:
            response = self.dashscope.TextEmbedding.call(
                model=self.model,
                input=[text],
                text_type="query"
            )
            if response and hasattr(response, 'output') and hasattr(response.output, 'embeddings'):
                if response.output.embeddings and len(response.output.embeddings) > 0:
                    return response.output.embeddings[0].embedding
            logger.error("è·å–query embeddingå¤±è´¥")
            return []
        except Exception as e:
            logger.error(f"DashScope query embeddingè°ƒç”¨å¤±è´¥: {e}")
            return []

@dataclass
class RetrievalConfig:
    """æ£€ç´¢é…ç½®ç±»"""
    title_weight: float = 0.5
    question_weight: float = 0.35
    answer_weight: float = 0.15
    
    def normalize_weights(self):
        """æ ‡å‡†åŒ–æƒé‡"""
        total = self.title_weight + self.question_weight + self.answer_weight
        if total > 0:
            self.title_weight /= total
            self.question_weight /= total
            self.answer_weight /= total

class TextChunk:
    """æ–‡æœ¬å—ç±»"""
    def __init__(self, content: str, chunk_type: str, original_index: int):
        self.content = content
        self.chunk_type = chunk_type  # 'title', 'question', 'answer'
        self.original_index = original_index

class DataRecord:
    """æ•°æ®è®°å½•ç±»"""
    def __init__(self, index: int, questionTitle: str, questionText: str, answerText: str):
        self.index = index
        self.questionTitle = questionTitle
        self.questionText = questionText
        self.answerText = answerText
        self.questionTitle_chunks = []
        self.questionText_chunks = []
        self.answerText_chunks = []

class QwenRAGSystem:
    """åŸºäºé€šä¹‰åƒé—®çš„RAGç³»ç»Ÿ"""
    
    def __init__(
        self,
        embedding_model: str = "text-embedding-v1",
        llm_model: str = "qwen-turbo",
        chunk_size: int = 500,
        chunk_overlap: int = 100,
        retrieval_config: Optional[RetrievalConfig] = None
    ):
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿ
        
        Args:
            embedding_model: embeddingæ¨¡å‹åç§°
            llm_model: LLMæ¨¡å‹åç§°
            chunk_size: æ–‡æœ¬åˆ‡ç‰‡å¤§å°
            chunk_overlap: æ–‡æœ¬é‡å å¤§å°
            retrieval_config: æ£€ç´¢é…ç½®
        """
        self.embedding_model = embedding_model
        self.llm_model = llm_model
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # åˆå§‹åŒ–é…ç½®
        self.retrieval_config = retrieval_config or RetrievalConfig()
        self.retrieval_config.normalize_weights()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_components()
        
        # æ•°æ®å­˜å‚¨
        self.data_records: List[DataRecord] = []
        self.vectorstore = None
        
    def _init_components(self):
        """åˆå§‹åŒ–ç»„ä»¶"""
        try:
            # æ£€æŸ¥é˜¿é‡Œäº‘é…ç½®
            self._check_alibaba_cloud_config()
            
            # åˆå§‹åŒ–embeddingå’Œllm - ä¸“é—¨ä¸ºé˜¿é‡Œäº‘é€šä¹‰åƒé—®è®¾è®¡
            if not ALIBABA_CLOUD_AVAILABLE:
                raise RuntimeError("é˜¿é‡Œäº‘SDKä¸å¯ç”¨ï¼Œæ— æ³•åˆå§‹åŒ–é€šä¹‰åƒé—®RAGç³»ç»Ÿ")
            
            # ä½¿ç”¨ DashScope embedding wrapper
            self.embeddings = DashScopeEmbeddingWrapper(model=self.embedding_model)
            self.dashscope = dashscope
            logger.info(f"ä½¿ç”¨é˜¿é‡Œäº‘é€šä¹‰åƒé—®æ¨¡å‹: {self.llm_model}")
            
            # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap,
                length_function=len,
                separators=["\n\n", "\n", " ", ""]
            )
            
            logger.info("ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _check_alibaba_cloud_config(self):
        """æ£€æŸ¥é˜¿é‡Œäº‘é…ç½®"""
        api_key = os.getenv('DASHSCOPE_API_KEY')
        if not api_key:
            logger.warning("æœªè®¾ç½®DASHSCOPE_API_KEYç¯å¢ƒå˜é‡")
            if not ALIBABA_CLOUD_AVAILABLE:
                raise ValueError("éœ€è¦è®¾ç½®é˜¿é‡Œäº‘APIå¯†é’¥æˆ–å®‰è£…ç›¸å…³SDK")
        
        # æ£€æŸ¥region
        region = os.getenv('ALIBABA_CLOUD_REGION', 'cn-beijing')
        logger.info(f"ä½¿ç”¨é˜¿é‡Œäº‘åŒºåŸŸ: {region}")
    
    def _semantic_chunk_text(self, text: str, chunk_type: str, record_index: int) -> List[TextChunk]:
        """è¯­ä¹‰åˆ‡ç‰‡æ–‡æœ¬"""
        if not text or text.strip() == "":
            return [TextChunk("", chunk_type, record_index)]
        
        try:
            # ä½¿ç”¨langchainçš„æ–‡æœ¬åˆ†å‰²å™¨
            splits = self.text_splitter.split_text(text)
            return [TextChunk(split, chunk_type, record_index) for split in splits]
        except Exception as e:
            logger.warning(f"æ–‡æœ¬åˆ‡ç‰‡å¤±è´¥: {e}, ä½¿ç”¨åŸå§‹æ–‡æœ¬")
            return [TextChunk(text, chunk_type, record_index)]
    
    def _create_embeddings_for_chunks(self, chunks: List[TextChunk]) -> List[np.ndarray]:
        """ä¸ºæ–‡æœ¬å—åˆ›å»ºembedding"""
        try:
            texts = [chunk.content for chunk in chunks]
            embeddings = self.embeddings.embed_documents(texts)
            return [np.array(emb) for emb in embeddings]
        except Exception as e:
            logger.error(f"åˆ›å»ºembeddingå¤±è´¥: {e}")
            return []
    
    def load_and_process_data(self, data_path: str) -> bool:
        """
        åŠ è½½å’Œå¤„ç†æ•°æ®
        
        Args:
            data_path: JSONæ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            # åŠ è½½JSONæ•°æ®
            with open(data_path, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
            
            logger.info(f"åŠ è½½äº† {len(raw_data)} æ¡åŸå§‹æ•°æ®")
            #MAX_RECORDS = 10000
            # å¤„ç†æ¯æ¡è®°å½•
            for i, item in enumerate(raw_data):
                try:
                    # åˆ›å»ºæ•°æ®è®°å½•
                    record = DataRecord(
                        index=i,
                        questionTitle=item.get('questionTitle', ''),
                        questionText=item.get('questionText', ''),
                        answerText=item.get('answerText', '')
                    )
                    
                    # è¯­ä¹‰åˆ‡ç‰‡
                    record.questionTitle_chunks = self._semantic_chunk_text(
                        record.questionTitle, 'title', i
                    )
                    record.questionText_chunks = self._semantic_chunk_text(
                        record.questionText, 'question', i
                    )
                    record.answerText_chunks = self._semantic_chunk_text(
                        record.answerText, 'answer', i
                    )
                    
                    self.data_records.append(record)
                    
                except Exception as e:
                    logger.warning(f"å¤„ç†ç¬¬ {i} æ¡è®°å½•å¤±è´¥: {e}")
                    continue
            
            logger.info(f"æˆåŠŸå¤„ç†äº† {len(self.data_records)} æ¡è®°å½•")
            return True
            
        except Exception as e:
            logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False
    #####

    def init_vectorstore(self):
        logger.info("ã€INITã€‘è¿›å…¥ init_vectorstore")

        embedding_model = DashScopeEmbeddings(model="text-embedding-v1")
        VECTORSTORE_DIR = "./vectorstore/faiss_qwen"

        if os.path.exists(VECTORSTORE_DIR):
            logger.info("ã€INITã€‘æ£€æµ‹åˆ°å·²æœ‰å‘é‡æ•°æ®åº“ï¼Œç›´æ¥åŠ è½½")
            self.vectorstore = FAISS.load_local(
                VECTORSTORE_DIR,
                embedding_model,
                allow_dangerous_deserialization=True
            )
            return True

        logger.info("ã€INITã€‘æœªæ£€æµ‹åˆ°å‘é‡æ•°æ®åº“ï¼Œå¼€å§‹æ„å»º")
        success = self.build_vectorstore()
        if not success:
            logger.error("ã€INITã€‘build_vectorstore å¤±è´¥")
            return False

        os.makedirs(VECTORSTORE_DIR, exist_ok=True)
        self.vectorstore.save_local(VECTORSTORE_DIR)
        logger.info(f"ã€INITã€‘å‘é‡æ•°æ®åº“å·²ä¿å­˜åˆ° {VECTORSTORE_DIR}")

        return True

    #######
    def build_vectorstore(self) -> bool:
        """
        æ„å»ºå‘é‡æ•°æ®åº“ï¼ˆç¨³å®šç‰ˆï¼‰
        - åˆ†æ‰¹ embedding
        - ä¸¥æ ¼æ ¡éªŒ
        - ç»Ÿä¸€ embedding å¯¹è±¡
        """
        try:
            all_chunks = []
            metadatas = []

            # ========= 1. æ”¶é›†æ–‡æœ¬å— =========
            for record in self.data_records:
                for chunk in (
                        record.questionTitle_chunks
                        + record.questionText_chunks
                        + record.answerText_chunks
                ):
                    text = chunk.content.strip()
                    if not text:
                        continue

                    all_chunks.append(text)
                    metadatas.append({
                        "record_index": record.index,
                        "chunk_type": chunk.chunk_type,
                        "original_text": text[:100] + "..." if len(text) > 100 else text
                    })

            if not all_chunks:
                logger.error("æ²¡æœ‰æœ‰æ•ˆæ–‡æœ¬å—ï¼Œå‘é‡æ•°æ®åº“æ„å»ºç»ˆæ­¢")
                return False

            logger.info(f"å…±æ”¶é›† {len(all_chunks)} ä¸ªæ–‡æœ¬å—ï¼Œå¼€å§‹åˆ›å»º embedding...")

            # ========= 2. åˆå§‹åŒ– embedding æ¨¡å‹ =========
            embedding_model = DashScopeEmbeddings(model="text-embedding-v1")

            # ========= 3. åˆ†æ‰¹ embedding =========
            import concurrent.futures
            import time

            def batch_embed(texts, batch_size=8, timeout=30):
                all_embeddings = []

                for start in range(0, len(texts), batch_size):
                    batch = texts[start:start + batch_size]

                    logger.info(f"Embedding batch [{start}:{start + len(batch)}]")

                    try:
                        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                            future = executor.submit(
                                embedding_model.embed_documents, batch
                            )
                            batch_embeddings = future.result(timeout=timeout)

                    except concurrent.futures.TimeoutError:
                        raise RuntimeError(
                            f"Embedding è¶…æ—¶ï¼ˆ>{timeout}sï¼‰ï¼Œbatch èµ·å§‹ç´¢å¼• {start}"
                        )

                    except Exception as e:
                        raise RuntimeError(
                            f"Embedding è°ƒç”¨å¼‚å¸¸ï¼Œbatch èµ·å§‹ç´¢å¼• {start}"
                        ) from e

                    if not batch_embeddings or len(batch_embeddings) != len(batch):
                        raise RuntimeError(
                            f"Embedding æ•°é‡å¼‚å¸¸ï¼Œbatch èµ·å§‹ç´¢å¼• {start}"
                        )

                    all_embeddings.extend(batch_embeddings)

                    time.sleep(0.1)  # ğŸ”¥ éå¸¸é‡è¦ï¼šä¸»åŠ¨é™é€Ÿï¼Œé¿å… QPS é™æµ

                return all_embeddings

            embeddings = batch_embed(all_chunks, batch_size=32)

            logger.info("Embedding åˆ›å»ºå®Œæˆï¼Œå¼€å§‹æ„å»º FAISS å‘é‡æ•°æ®åº“...")

            # ========= 4. æ„å»º FAISS =========
            text_embeddings = list(zip(all_chunks, embeddings))

            self.vectorstore = FAISS.from_embeddings(
                text_embeddings=text_embeddings,
                embedding=embedding_model,
                metadatas=metadatas
            )

            logger.info("å‘é‡æ•°æ®åº“æ„å»ºæˆåŠŸ")
            return True

        except Exception as e:
            logger.exception(f"å‘é‡æ•°æ®åº“æ„å»ºå¤±è´¥: {e}")
            return False
    
    # def _calculate_cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
    #     """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    #     try:
    #         dot_product = np.dot(vec1, vec2)
    #         norm1 = np.linalg.norm(vec1)
    #         norm2 = np.linalg.norm(vec2)
    #         if norm1 == 0 or norm2 == 0:
    #             return 0.0
    #         return float(dot_product / (norm1 * norm2))
    #     except:
    #         return 0.0


    def _query_rewrite(self, query: str):
        """
        æŸ¥è¯¢é‡å†™æ¥å£ï¼š
        - å°†æŸ¥è¯¢æ‹†åˆ†ä¸ºå¤šä¸ªå­é—®é¢˜
        - ä¸ºæ¯ä¸ªå­é—®é¢˜åˆ†é…æƒé‡
        - è¿”å›ç»“æ„åŒ–ç»“æœï¼Œä¾¿äºç›¸ä¼¼åº¦è®¡ç®—
        """

        query = query.strip()
        if not query:
            return {
                "original_query": "",
                "sub_queries": []
            }

        prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢é‡å†™ä¸æ„å›¾æ‹†è§£ç³»ç»Ÿã€‚

    ä½ çš„ä»»åŠ¡æ˜¯ï¼š
    1. å°†ç”¨æˆ·æŸ¥è¯¢æ‹†åˆ†ä¸ºè‹¥å¹²ã€Œå­é—®é¢˜ã€
    2. åˆ¤æ–­æ¯ä¸ªå­é—®é¢˜åœ¨æ•´ä½“æŸ¥è¯¢ä¸­çš„é‡è¦æ€§
    3. ä¸ºæ¯ä¸ªå­é—®é¢˜åˆ†é…ä¸€ä¸ªæƒé‡ï¼ˆ0~1ï¼Œæƒé‡ä¹‹å’Œä¸º 1ï¼‰
    4. è¾“å‡ºä¸¥æ ¼çš„ JSON æ ¼å¼ï¼Œä¾¿äºç¨‹åºè§£æ

    æ‹†åˆ†åŸåˆ™ï¼š
    - å­é—®é¢˜è¦å°½é‡çŸ­ã€æ˜ç¡®ã€å¯ç‹¬ç«‹ç”¨äºå‘é‡æ£€ç´¢
    - ç¬¬ä¸€ä¸ªå­é—®é¢˜é€šå¸¸æ˜¯æ ¸å¿ƒé—®é¢˜
    - ä¸è¦ç”Ÿæˆæ— å…³æˆ–æ¨æµ‹æ€§é—®é¢˜

    è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
    {{
      "original_query": "...",
      "sub_queries": [
        {{
          "text": "å­é—®é¢˜1",
          "weight": 0.5
        }},
        {{
          "text": "å­é—®é¢˜2",
          "weight": 0.3
        }},
        {{
          "text": "å­é—®é¢˜3",
          "weight": 0.2
        }}
      ]
    }}

    ç”¨æˆ·æŸ¥è¯¢ï¼š
    \"\"\"{query}\"\"\"
    """

        try:
            response = self.dashscope.Generation.call(
                model=self.llm_model,
                prompt=prompt,
                temperature=0.1,
                max_tokens=2000
            )

            content = response["output"]["text"]
            result = json.loads(content)

            # åŸºæœ¬æ ¡éªŒï¼Œé˜²æ­¢æ¨¡å‹ä¹±è¾“å‡º
            if "sub_queries" not in result:
                raise ValueError("Missing sub_queries field")

            return result

        except Exception as e:
            logger.warning(f"Query rewrite failed, fallback to original query: {e}")

            # å…œåº•ï¼šä¸æ‹†åˆ†ï¼Œæƒé‡ä¸º 1
            return {
                "original_query": query,
                "sub_queries": [
                    {
                        "text": query,
                        "weight": 1.0
                    }
                ]
            }

    def _adaptive_similarity_weights(self, query: str, record: DataRecord) -> Tuple[float, float, float]:
        """
        è‡ªé€‚åº”ç›¸ä¼¼åº¦æƒé‡æ¥å£ï¼ˆå¯æ‰©å±•ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            record: æ•°æ®è®°å½•
            
        Returns:
            Tuple[float, float, float]: è‡ªé€‚åº”çš„æƒé‡
        """
        # è¿™é‡Œå¯ä»¥æ ¹æ®æŸ¥è¯¢ç‰¹å¾åŠ¨æ€è°ƒæ•´æƒé‡
        # å½“å‰è¿”å›é»˜è®¤æƒé‡
        return self.retrieval_config.title_weight, \
               self.retrieval_config.question_weight, \
               self.retrieval_config.answer_weight


    def search(self, query: str, top_k: int = 5):

        def translate_to_english(text):
            """
            ä½¿ç”¨qwen-turbo APIå°†ä¸­æ–‡æ–‡æœ¬ç¿»è¯‘ä¸ºè‹±æ–‡
            :param text: å¾…ç¿»è¯‘çš„ä¸­æ–‡æ–‡æœ¬
            :return: ç¿»è¯‘åçš„è‹±æ–‡æ–‡æœ¬ï¼ˆå¤±è´¥è¿”å›åŸæ–‡æœ¬ï¼‰
            """
            # ç©ºæ–‡æœ¬ç›´æ¥è¿”å›
            if not text or text.strip() == "":
                logger.warning("å¾…ç¿»è¯‘æ–‡æœ¬ä¸ºç©ºï¼Œç›´æ¥è¿”å›")
                return text

            # æ„é€ ç¿»è¯‘æç¤ºè¯ï¼ˆç²¾å‡†å¼•å¯¼qwenè¿”å›çº¯è‹±æ–‡ç¿»è¯‘ç»“æœï¼‰
            prompt = f"""
            ä½ çš„å”¯ä¸€ä»»åŠ¡æ˜¯å°†ä»¥ä¸‹æ–‡æœ¬ç²¾å‡†ç¿»è¯‘ä¸ºåœ°é“çš„è‹±æ–‡ï¼Œæ— éœ€é¢å¤–è§£é‡Šã€è¯´æ˜æˆ–è¡¥å……å†…å®¹ï¼Œä»…è¿”å›ç¿»è¯‘ç»“æœï¼š
            å¾…ç¿»è¯‘æ–‡æœ¬ï¼š{text}
            """

            try:
                # è°ƒç”¨qwen-turbo API
                response: GenerationResponse = Generation.call(
                    model=self.llm_model,
                    messages=[
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    result_format='text',  # ç›´æ¥è¿”å›æ–‡æœ¬ç»“æœï¼Œæ— éœ€è§£æJSON
                    temperature=0.1,  # ä½æ¸©åº¦ä¿è¯ç¿»è¯‘ç»“æœç¨³å®š
                    max_tokens=1024,  # é™åˆ¶è¿”å›é•¿åº¦ï¼Œé€‚é…çŸ­æ–‡æœ¬ç¿»è¯‘
                    timeout=10  # è®¾ç½®è¶…æ—¶æ—¶é—´ï¼Œé¿å…ç¨‹åºå¡ä½
                )

                # è§£æå“åº”ç»“æœ
                if response.status_code == 200:
                    translated_text = response.output.text.strip()
                    logger.info(f"ç¿»è¯‘æˆåŠŸï¼š{text[:100]} -> {translated_text[:100]}")
                    return translated_text
                else:
                    logger.error(f"qwen-turbo APIè°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}ï¼Œä¿¡æ¯ï¼š{response.message}")
                    return text

            except Exception as e:
                logger.error(f"ç¿»è¯‘å¼‚å¸¸ï¼š{text}ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{str(e)}")
                return text

        try:
            rewritten = self._query_rewrite(query)
            logger.info(f"åŸå§‹æŸ¥è¯¢: {query}")
            logger.info(f"é‡å†™ç»“æœ: {rewritten}")
            if isinstance(rewritten, str):
                rewritten = {
                    "original_query": rewritten,
                    "sub_queries": [
                        {"text": rewritten, "weight": 1.0}
                    ]
                }

            sub_queries = rewritten.get("sub_queries", [])
            # åŸå§‹æŸ¥è¯¢çš„å­æŸ¥è¯¢é¡¹
            original_sub_query = {"text": query, "weight": 0.5}

            total_weight = sum(s.get("weight", 0) for s in sub_queries)
            if total_weight > 0:
                for s in sub_queries:
                    s["weight"] = (s["weight"] / total_weight) * 0.5

            sub_queries.insert(0, original_sub_query)

            for idx, sub_query in enumerate(sub_queries):
                original_text = sub_query.get("text", "")
                if original_text:
                    translated_text = translate_to_english(original_text)
                    sub_query["text"] = translated_text  # æ›¿æ¢ä¸ºè‹±æ–‡

            record_scores = {}
            seen_chunks = {}

            for sub in sub_queries:
                sub_query = sub["text"]
                weight = sub["weight"]
                sub_top_k = max(1, int(round(15 * weight)))

                docs_with_scores = self.vectorstore.similarity_search_with_score(
                    sub_query,
                    k=sub_top_k
                )

                for doc, score in docs_with_scores:
                    metadata = doc.metadata
                    record_index = metadata["record_index"]
                    chunk_type = metadata["chunk_type"]
                    chunk_id = metadata.get("chunk_id", id(doc))

                    # â­ æ„é€ å”¯ä¸€ chunk key
                    chunk_key = (record_index, chunk_type, chunk_id)

                    similarity = (1.0 / (1.0 + score)) * 100000
                    weighted_similarity = similarity * weight

                    # â­ å»é‡é€»è¾‘ï¼šåªä¿ç•™æœ€å¤§è´¡çŒ®
                    if chunk_key in seen_chunks:
                        if weighted_similarity <= seen_chunks[chunk_key]:
                            continue
                        else:
                            seen_chunks[chunk_key] = weighted_similarity
                    else:
                        seen_chunks[chunk_key] = weighted_similarity

                    if record_index not in record_scores:
                        record_scores[record_index] = {
                            "record": self.data_records[record_index],
                            "title_similarities": [],
                            "question_similarities": [],
                            "answer_similarities": []
                        }

                    if chunk_type == "title":
                        record_scores[record_index]["title_similarities"].append(weighted_similarity)
                    elif chunk_type == "question":
                        record_scores[record_index]["question_similarities"].append(weighted_similarity)
                    elif chunk_type == "answer":
                        record_scores[record_index]["answer_similarities"].append(weighted_similarity)

            final_scores = []

            for record_index, data in record_scores.items():
                title_avg = np.mean(data["title_similarities"]) if data["title_similarities"] else 0.0
                question_avg = np.mean(data["question_similarities"]) if data["question_similarities"] else 0.0
                answer_avg = np.mean(data["answer_similarities"]) if data["answer_similarities"] else 0.0

                title_w, question_w, answer_w = self._adaptive_similarity_weights(
                    query, data["record"]
                )

                final_similarity = (
                        title_w * title_avg +
                        question_w * question_avg +
                        answer_w * answer_avg
                )

                final_scores.append({
                    "record_index": record_index,
                    "record": data["record"],
                    "final_similarity": final_similarity,
                    "title_similarity": title_avg,
                    "question_similarity": question_avg,
                    "answer_similarity": answer_avg,
                    "title_weight": title_w,
                    "question_weight": question_w,
                    "answer_weight": answer_w
                })

            final_scores.sort(key=lambda x: x["final_similarity"], reverse=True)



            return final_scores[:top_k]

        except Exception as e:
            logger.exception(f"æœç´¢å¤±è´¥: {e}")
            return []

    def generate_response(self, query: str, top_k: int = 5) -> str:
        """
        ç”Ÿæˆå›ç­”
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: ä½¿ç”¨çš„å€™é€‰æ–‡æ¡£æ•°é‡
            
        Returns:
            str: ç”Ÿæˆçš„å›ç­”
        """
        try:
            # æ£€ç´¢ç›¸å…³æ–‡æ¡£
            search_results = self.search(query, top_k)
            
            if not search_results:
                return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„å›ç­”ã€‚"
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context_parts = []
            for i, result in enumerate(search_results, 1):
                record = result['record']
                context_parts.append(f"å€™é€‰ {i} (ç›¸ä¼¼åº¦: {result['final_similarity']:.3f}):")
                context_parts.append(f"é—®é¢˜: {record.questionTitle}")
                if record.questionText:
                    context_parts.append(f"é—®é¢˜è¯¦æƒ…: {record.questionText}")
                context_parts.append(f"å›ç­”: {record.answerText}")
                context_parts.append("-" * 50)
            
            context = "\n".join(context_parts)

            # æ„å»ºæç¤ºè¯
            prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¯·æ ¹æ®ä¸Šä¸‹æ–‡å†…å®¹æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚

            ä¸Šä¸‹æ–‡ä¿¡æ¯:
            {context}

            ç”¨æˆ·é—®é¢˜: {query}

            è¯·åŸºäºä¸Šè¿°ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯´æ˜å¹¶æä¾›ä¸€èˆ¬æ€§çš„å»ºè®®ã€‚
            """

            # ç”Ÿæˆå›ç­”
            if ALIBABA_CLOUD_AVAILABLE and hasattr(self, 'dashscope'):
                try:
                    response = self.dashscope.Generation.call(
                        model=self.llm_model,
                        prompt=prompt,
                        temperature=0.1,
                        max_tokens=2000
                    )

                    if response and hasattr(response, 'output') and hasattr(response.output, 'text'):
                        return response.output.text.strip()
                    else:
                        return "æŠ±æ­‰ï¼Œæ¨¡å‹ç”Ÿæˆå¤±è´¥"

                except Exception as e:
                    logger.exception(f"DashScopeè°ƒç”¨å¤±è´¥: {e}")
                    return f"æŠ±æ­‰ï¼Œæ¨¡å‹è°ƒç”¨å¤±è´¥: {e}"
            else:
                best_result = search_results[0]
                return f"åŸºäºç›¸ä¼¼åº¦æœ€é«˜çš„å›ç­”ï¼š\n{best_result['record'].answerText}"


        except Exception as e:
            logger.error(f"ç”Ÿæˆå›ç­”å¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œå›ç­”ç”Ÿæˆå¤±è´¥: {e}"

    def get_retrieval_stats(self) -> Dict[str, Any]:
        """è·å–æ£€ç´¢ç»Ÿè®¡ä¿¡æ¯"""
        if not self.data_records:
            return {}
        
        total_records = len(self.data_records)
        total_chunks = sum(
            len(record.questionTitle_chunks) + 
            len(record.questionText_chunks) + 
            len(record.answerText_chunks)
            for record in self.data_records
        )
        
        return {
            'total_records': total_records,
            'total_chunks': total_chunks,
            'avg_chunks_per_record': total_chunks / total_records if total_records > 0 else 0,
            'chunk_size': self.chunk_size,
            'chunk_overlap': self.chunk_overlap,
            'retrieval_config': {
                'title_weight': self.retrieval_config.title_weight,
                'question_weight': self.retrieval_config.question_weight,
                'answer_weight': self.retrieval_config.answer_weight
            },
            'embedding_model': self.embedding_model,
            'llm_model': self.llm_model,
            'using_alibaba_cloud': ALIBABA_CLOUD_AVAILABLE
        }

def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    # åˆ›å»ºRAGç³»ç»Ÿ
    rag = QwenRAGSystem(
        embedding_model="text-embedding-v1",
        llm_model="qwen-turbo",
        chunk_size=500,
        chunk_overlap=100,
        retrieval_config=RetrievalConfig(title_weight=0.5, question_weight=0.35, answer_weight=0.15)
    )
    
    # æ•°æ®è·¯å¾„
    data_path = r"E:\1__xubin_hu\Program and setting\datasets\Mental_Health_conv\cl_output_file.json"
    
    # åŠ è½½å’Œå¤„ç†æ•°æ®
    print("æ­£åœ¨åŠ è½½å’Œå¤„ç†æ•°æ®...")
    if not rag.load_and_process_data(data_path):
        print("æ•°æ®åŠ è½½å¤±è´¥")
        return
    
    # æ„å»ºå‘é‡æ•°æ®åº“
    print("æ­£åœ¨æ„å»ºå‘é‡æ•°æ®åº“...")
    ok = rag.init_vectorstore()
    print("init_vectorstore è¿”å›å€¼:", ok)

    if not ok:
        print("å‘é‡æ•°æ®åº“æ„å»ºå¤±è´¥")
        return

    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = rag.get_retrieval_stats()
    print("\nç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯:")
    for key, value in stats.items():
        print(f"{key}: {value}")
    
     # æœç´¢æµ‹è¯•
    query = "ä»€ä¹ˆæ˜¯ææ…Œå‘ä½œï¼Ÿ"
    print(f"\næµ‹è¯•æŸ¥è¯¢: {query}")

    # æ£€ç´¢ç›¸å…³æ–‡æ¡£
    search_results = rag.search(query, top_k=3)
    print(f"\næ‰¾åˆ° {len(search_results)} ä¸ªç›¸å…³ç»“æœ:")
    for i, result in enumerate(search_results, 1):
        print(f"\nç»“æœ {i}:")
        print(f"ç›¸ä¼¼åº¦: {result['final_similarity']:.3f}")
        print(f"é—®é¢˜: {result['record'].questionTitle}")
        print(f"å›ç­”é¢„è§ˆ: {result['record'].answerText[:100]}...")
    
    # ç”Ÿæˆå®Œæ•´å›ç­”
    print("\n" + "="*80)
    print("ç”Ÿæˆçš„å›ç­”:")
    print("="*80)
    response = rag.generate_response(query, top_k=3)
    print(response)

if __name__ == "__main__":
    main()